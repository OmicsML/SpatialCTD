{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"QwKFm-xUxJw7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679178645873,"user_tz":420,"elapsed":23999,"user":{"displayName":"jiayuan ding","userId":"00368278201421210170"}},"outputId":"b6584244-ea13-4cc1-d705-db694cce1b41"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VzUmZryBZS4N"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kTuURbGRr889","outputId":"913e6522-a7ac-409d-99cf-a377c6f7af84","executionInfo":{"status":"ok","timestamp":1679178659535,"user_tz":420,"elapsed":13666,"user":{"displayName":"jiayuan ding","userId":"00368278201421210170"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ipdb\n","  Downloading ipdb-0.13.13-py3-none-any.whl (12 kB)\n","Requirement already satisfied: tomli in /usr/local/lib/python3.9/dist-packages (from ipdb) (2.0.1)\n","Collecting ipython>=7.31.1\n","  Downloading ipython-8.11.0-py3-none-any.whl (793 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m793.3/793.3 KB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipdb) (4.4.2)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n","Collecting matplotlib-inline\n","  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n","Collecting jedi>=0.16\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: traitlets>=5 in /usr/local/lib/python3.9/dist-packages (from ipython>=7.31.1->ipdb) (5.7.1)\n","Collecting stack-data\n","  Downloading stack_data-0.6.2-py3-none-any.whl (24 kB)\n","Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=7.31.1->ipdb) (2.6.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n","Collecting prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30\n","  Downloading prompt_toolkit-3.0.38-py3-none-any.whl (385 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 KB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython>=7.31.1->ipdb) (4.8.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.31.1->ipdb) (0.2.6)\n","Collecting pure-eval\n","  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n","Collecting executing>=1.2.0\n","  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n","Collecting asttokens>=2.1.0\n","  Downloading asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=7.31.1->ipdb) (1.15.0)\n","Installing collected packages: pure-eval, executing, prompt-toolkit, matplotlib-inline, jedi, asttokens, stack-data, ipython, ipdb\n","  Attempting uninstall: prompt-toolkit\n","    Found existing installation: prompt-toolkit 2.0.10\n","    Uninstalling prompt-toolkit-2.0.10:\n","      Successfully uninstalled prompt-toolkit-2.0.10\n","  Attempting uninstall: ipython\n","    Found existing installation: ipython 7.9.0\n","    Uninstalling ipython-7.9.0:\n","      Successfully uninstalled ipython-7.9.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 8.11.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed asttokens-2.2.1 executing-1.2.0 ipdb-0.13.13 ipython-8.11.0 jedi-0.18.2 matplotlib-inline-0.1.6 prompt-toolkit-3.0.38 pure-eval-0.2.2 stack-data-0.6.2\n"]}],"source":["!pip install ipdb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kjigXpBjboUu","outputId":"6c46552f-bd32-4b10-fb39-e424258ff6cd","executionInfo":{"status":"ok","timestamp":1679178691000,"user_tz":420,"elapsed":31469,"user":{"displayName":"jiayuan ding","userId":"00368278201421210170"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.1%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (9.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch-scatter\n","Successfully installed torch-scatter-2.1.1+pt113cu116\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n","Collecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.17%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (4.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-sparse) (1.10.1)\n","Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy->torch-sparse) (1.22.4)\n","Installing collected packages: torch-sparse\n","Successfully installed torch-sparse-0.6.17+pt113cu116\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n","Collecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_cluster-1.6.1%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (3.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-cluster) (1.10.1)\n","Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy->torch-cluster) (1.22.4)\n","Installing collected packages: torch-cluster\n","Successfully installed torch-cluster-1.6.1+pt113cu116\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n","Collecting torch-spline-conv\n","  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_spline_conv-1.2.2%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (868 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.4/868.4 KB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch-spline-conv\n","Successfully installed torch-spline-conv-1.2.2+pt113cu116\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 KB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (4.65.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.10.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (2.27.1)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.0.9)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (5.9.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch-geometric) (2.1.2)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (3.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (1.1.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=f95fdb7633d68c37d1e1b0733eb395aaa2916007fbc88fd18033d6fc9e3e0258\n","  Stored in directory: /root/.cache/pip/wheels/31/b2/8c/9b4bb72a4384eabd1ffeab2b7ead692c9165e35711f8a9dc72\n","Successfully built torch-geometric\n","Installing collected packages: torch-geometric\n","Successfully installed torch-geometric-2.2.0\n"]}],"source":["# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n","import torch\n","\n","def format_pytorch_version(version):\n","  return version.split('+')[0]\n","\n","TORCH_version = torch.__version__\n","TORCH = format_pytorch_version(TORCH_version)\n","\n","def format_cuda_version(version):\n","  return 'cu' + version.replace('.', '')\n","\n","CUDA_version = torch.version.cuda\n","CUDA = format_cuda_version(CUDA_version)\n","\n","!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-geometric "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U4X4FHNo-JyZ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics.pairwise import euclidean_distances\n","from torch_geometric.data import Data\n","from scipy import sparse\n","import random\n","import torch\n","import math\n","from tqdm.autonotebook import tqdm\n","import sklearn.metrics.pairwise as pairwise \n","from sklearn.metrics import pairwise_distances\n","from collections import defaultdict\n","from functools import lru_cache\n","from collections import defaultdict\n","\n","\n","def get_X_Y_data_coordiate(file_path, ground_path, layout_row = 4, spot_in_fov=20, spot_in_fov_col=4):    \n","    spot_gene_expression = pd.read_csv(file_path)\n","    maxfov = spot_gene_expression['fov'].max()\n","    row = math.ceil(maxfov / layout_row)\n","    right_top = row * layout_row\n","    gene_exp_matrix = [[[0,0] for _ in range(spot_in_fov)] for _ in range(maxfov)]\n","    for fov in range(1,maxfov+1):\n","        start_point = (((maxfov - fov) // layout_row) * 4, ((layout_row -1) - (right_top-fov) % layout_row) * 5 )\n","        for spot in range(1,spot_in_fov+1):\n","            # offset according with 1\n","            gene_exp_matrix[fov-1][spot-1][0] = start_point[0] + (spot-1) % spot_in_fov_col\n","            gene_exp_matrix[fov-1][spot-1][1] = start_point[1] + (spot-1)// spot_in_fov_col\n","    gene_expression = spot_gene_expression.to_numpy()[:,3:]\n","    gene_exp_matrix = np.array(gene_exp_matrix).reshape(-1,2)\n","    label = pd.read_csv(ground_path)\n","    label = label.to_numpy()[:,3:]\n","    return gene_expression, gene_exp_matrix, label\n","      \n","\n","def remove_self_loop(adj, weight):\n","    not_mask = []\n","    for idx, i in enumerate(adj):\n","        if i[0] != i[1]:\n","            not_mask.append(idx)\n","    return adj[not_mask], weight[not_mask]\n","\n","def distance_matrix_threshold_2_adj(distance_matrix, threshold=5, self_loop=False):\n","    x, y = np.where(distance_matrix <= threshold)\n","    adj = np.vstack([x,y]).T\n","    weight = distance_matrix[x,y]\n","    if not self_loop:\n","        adj, weight = remove_self_loop(adj, weight)\n","    \n","    return adj, weight\n","\n","\n","def get_adj(data, Y=None, topk=5, distance_metric=\"l2\", threshold=None):\n","    # if threshold has given, topk will not work \n","    distance_matrix = pairwise_distances(data, Y=Y, metric=distance_metric)\n","    if not threshold:\n","        adj, weight = distance_matrix_topk_2_adj(distance_matrix, topk)\n","    else:\n","        adj, weight = distance_matrix_threshold_2_adj(distance_matrix, threshold=threshold)\n","    return adj, weight\n","\n","def array_2_np_index(idx_arr):\n","    res = []\n","    for e,i in enumerate(idx_arr):\n","        res += [(e,j) for j in i if e!=j]\n","    res = np.array(res)\n","    idx, jdx = res[:,0], res[:,1]\n","    return idx, jdx\n","\n","def distance_matrix_topk_2_adj(distance_matrix,topk=5):\n","    \n","    sorted_arr = np.argsort(distance_matrix)[:,:topk]\n","    idx, jdx = array_2_np_index(sorted_arr)\n","    adj = np.vstack((idx,jdx)).T\n","    weight = distance_matrix[idx,jdx]\n","    return adj, weight\n","\n","def distance_matrix_normalize(adj):\n","    # handle adj_weight \n","    # adj_weight normlize into (0.1,1) \n","    # need attention : the minimun is the largest since the weight is the great\n","    link, link_weight = adj\n","    maxv = max(link_weight)\n","    minv = min(link_weight)\n","    link_weight = (maxv - link_weight) / (maxv - minv) + 0.1\n","    return link, link_weight\n","\n","def union_adj(adj_list, weight_list=[0.3,0.7]):\n","    memory = defaultdict(lambda :defaultdict(bool))\n","    memory_idx = defaultdict(lambda : defaultdict(int))\n","    memory_val = defaultdict(lambda : defaultdict(list))\n","    adj_link_total = []\n","    adj_weight_total = []\n","    for adj in adj_list:\n","        \n","        adj_link, adj_weight = adj\n","        for idx, dl in enumerate(adj_link):\n","            if memory[dl[0]][dl[1]] == True:\n","                # weight TODO \n","                memory_val[dl[0]][dl[1]].append(adj_weight[idx])\n","                temp_idx = memory_idx[dl[0]][dl[1]]\n","                cur_len = len(memory_val[dl[0]][dl[1]])\n","                if cur_len == 1:\n","                    adj_weight_total[temp_idx] = adj_weight_total[temp_idx] * weight_list[0] + adj_weight[idx]*weight_list[1]\n","                else:\n","                    adj_weight_total[temp_idx] += adj_weight[idx]*weight_list[cur_len]\n","                \n","            else:\n","                memory[dl[0]][dl[1]] = True \n","                adj_link_total.append(dl)\n","                memory_idx[dl[0]][dl[1]] = len(adj_link_total)\n","                adj_weight_total.append(adj_weight[idx])\n","    return np.array(adj_link_total), np.array(adj_weight_total)\n","\n","\n","class Graph:\n","    # genernal \n","    \n","    def __init__(self, graph = None, X=None, adj_link=None, adj_weight=None, label=None):\n","        # deep copy for each attribute \n","        if graph:\n","            self.X = graph.X.copy()\n","            self.adj_link = graph.adj_link.copy()\n","            self.adj_weight = graph.adj_weight.copy()\n","            # if graph.label is None:\n","            #     pass\n","            # else:\n","            self.label = graph.label.copy()\n","        else:\n","            self.X = X.copy()\n","            self.adj_link = adj_link.copy()\n","            self.adj_weight = adj_weight.copy()\n","  \n","            self.label = label.copy()\n","    \n","    def set_region(self,graph_region):\n","        self.graph_region = graph_region\n","    \n","    def set_label(self, label):\n","        self.label = label\n","    \n","    def __repr__(self,):\n","        return \"X:\"+str(self.X.shape)+\"; adj_link: \"+str(self.adj_link.shape)\n","      \n","    def __str__(self):\n","        return self.__repr__()\n","    \n","    def __hash__(self):\n","        pass\n","\n","def sample_fovs_graph(fpath, gpath, locationpath, topk=5, normalize=True, threshold=None):\n","    spot_gene_expression = pd.read_csv(fpath)\n","    X = spot_gene_expression.to_numpy()[:,3:]\n","\n","    label = pd.read_csv(gpath)\n","    label = label.to_numpy()[:,3:]\n","\n","    location = pd.read_csv(locationpath)\n","    cor = location.to_numpy()[:,3:]\n","\n","    spatial_adj = get_adj(cor, topk=topk, threshold=threshold)\n","    gene_adj = get_adj(X, topk=topk)\n","    if normalize:\n","        spatial_adj = distance_matrix_normalize(spatial_adj)\n","        gene_adj = distance_matrix_normalize(gene_adj)\n","    adj_link, adj_weight = union_adj([spatial_adj, gene_adj])\n","\n","    return Graph(X=X, adj_link=adj_link, adj_weight=adj_weight, label=label)\n","\n","def single_graph(fpath, gpath, topk=5, normalize=True, threshold=None, union_method=\"union\"):\n","    \n","    X, cor,label = get_X_Y_data_coordiate(fpath, gpath)\n","    spatial_adj = get_adj(cor, topk=topk, threshold=threshold)\n","    gene_adj = get_adj(X, topk=topk)\n","    \n","    if normalize:\n","        spatial_adj = distance_matrix_normalize(spatial_adj)\n","        gene_adj = distance_matrix_normalize(gene_adj)\n","    adj_link, adj_weight = union_adj([spatial_adj, gene_adj])\n","    # import ipdb\n","    # ipdb.set_trace()\n","\n","    return Graph(X=X, adj_link=adj_link, adj_weight=adj_weight, label=label)\n","\n","\n","def ingraph(pair, graph_region):\n","    # graph_region records the region of each graph \n","    #     such as [a,b,c,d]: the graph region is (0,a), (a,b), (b,c), (c,d) left concluded and right open \n","    prev = 0\n","    for region in graph_region:# will add additional loop \n","        if pair[0] < prev or pair[1] < prev:\n","            break\n","        if prev<=pair[0]<region and prev<=pair[1]<region:\n","            return True\n","        prev = region\n","    return False\n","\n","\n","def remove_inner_graph_link(adj_link, adj_weight, graph_region=[]):\n","    \n","    keep_idx = []\n","    for idx,pair in enumerate(adj_link):\n","        if not ingraph(pair, graph_region=graph_region):\n","            keep_idx.append(idx)\n","        \n","    return adj_link[keep_idx], adj_weight[keep_idx]\n","\n","def adj_add_region(adj_link, i_idx, j_idx , graph_region):\n","    left_offset = 0 if i_idx == 0 else graph_region[i_idx-1]\n","    right_offset= graph_region[j_idx-1]\n","    adj_link_list= adj_link.tolist()\n","    adj_link_list = [ [ left_offset + i[0], right_offset + i[1]] for i in adj_link_list]\n","    return np.array(adj_link_list)\n","\n","def multi_pair_distance(multi_players, \n","                        distance_metric=\"l2\",\n","                        topk=5, \n","                        threshold=None, \n","                        normalize=True,\n","                        graph_region=[],\n","                        ):\n","    \n","    # as the same as pair distance \n","    # complexity:  $n*(n-1) / 2$  $C_{n}^{2}$ but will accelerate by \n","    adj = [] # ((graph_id, node_id, grpah_id, node_id))\n","    weight = [] # just weight \n","    \n","    for graph_id_i in range(len(multi_players)):\n","        \n","        for graph_id_j in range(graph_id_i+1,len(multi_players)):\n","            \n","            temp_adj, temp_adj_weight = get_adj(multi_players[graph_id_i],\n","                                                multi_players[graph_id_j],\n","                                                distance_metric=distance_metric,\n","                                                threshold=threshold,\n","                                                topk = topk)\n","            \n","            temp_adj = adj_add_region(temp_adj, graph_id_i, graph_id_j, graph_region=graph_region )\n","            \n","            adj += temp_adj.tolist()\n","            weight += temp_adj_weight.tolist()\n","    \n","    adj, weight = np.array(adj), np.array(weight)\n","\n","    if normalize:\n","        adj, weight = distance_matrix_normalize((adj, weight))\n","\n","    return adj, weight\n","\n","\n","def union_graph(graph_list, \n","                method=\"link_expression\", \n","                topk=5, \n","                distance_metric='l2',\n","                threshold=None,\n","                union_method=\"point2point\"):\n","    \n","    # problem : \n","    #    1. union graph_list by method \n","    #    2. graph node number need modified (graph adj also need modified)\n","    #    3. \n","    \n","    # Solution : \n","    #    1. first link all graph and renum the nodes and adj\n","    #    2. use $method$ to add adj into huge adj \n","    \n","    if union_method not in [\"total\", \"point2point\"]:\n","        \n","        ValueError(\"union_method in union_graph now only support 'total' and 'point2point'\")\n","        \n","    \n","    union_graph = Graph(graph=graph_list[0])\n","    \n","    graph_region = []\n","    multi_players = [graph_list[0].X]\n","    # first renum each graph \n","    for graph in graph_list[1:]:\n","        \n","        multi_players.append(graph.X)\n","        \n","        prev = union_graph.X.shape[0]\n","        graph_region.append(prev)\n","        union_graph.adj_weight = np.concatenate([union_graph.adj_weight,graph.adj_weight])\n","        # renum node in adj \n","        temp_adj_link = graph.adj_link + prev\n","        temp_adj_link = np.concatenate([union_graph.adj_link, temp_adj_link])\n","        union_graph.adj_link = temp_adj_link\n","        \n","        # label \n","        union_graph.X = np.concatenate([union_graph.X, graph.X])\n","        union_graph.label = np.concatenate([union_graph.label, graph.label])\n","    \n","    graph_region.append(union_graph.X.shape[0])\n","    # use gene expression to add link \n","    \n","    if union_method == \"total\":\n","        # method 1 \n","        adj_link, adj_weight = get_adj(union_graph.X, topk=topk, distance_metric=distance_metric, threshold=threshold)\n","        adj_link, adj_weight = remove_inner_graph_link(adj_link, adj_weight, graph_region=graph_region)\n","        # remove the inner-graph link \n","    else:\n","        # method 2 \n","        \n","        # use graph_region to split graph \n","\n","        adj_link, adj_weight = multi_pair_distance(multi_players, \n","                                                   topk=topk, \n","                                                   distance_metric=distance_metric, \n","                                                   threshold=threshold, \n","                                                   graph_region=graph_region)\n","    \n","    adj_link, adj_weight  = distance_matrix_normalize((adj_link, adj_weight))\n","    union_graph.adj_link  = np.concatenate([union_graph.adj_link, adj_link]).T\n","    union_graph.adj_weight= np.concatenate([union_graph.adj_weight, adj_weight])\n","    # records region in graph\n","    union_graph.set_region(graph_region)\n","    return union_graph\n","\n","\n","def mask_graph(graph, masked_graph_idx=[0], return_validation=True, validate_split = 0.2):\n","    # take attention :  \n","    # mask generation : training: (appointed graph) test \n","    # train mask : random mask \n","    \n","    # note: masked_graph may be a list \n","    \n","    graph_region = graph.graph_region\n","    test_mask = []\n","    for masked_graph in masked_graph_idx:\n","        left = 0 if masked_graph == 0 else graph_region[masked_graph-1]\n","        right= graph_region[masked_graph]\n","        test_mask += [i for i in range(left, right)]\n","    \n","    remain_graph_idx = [i for i in range(len(graph_region)) if i not in masked_graph_idx]\n","    remain_mask = []\n","    for remain_graph in remain_graph_idx:\n","        left = 0 if remain_graph == 0 else graph_region[remain_graph-1]\n","        right= graph_region[remain_graph]\n","        remain_mask += [i for i in range(left, right)]\n","    \n","    if not return_validation:\n","        return test_mask, remain_mask, []\n","    \n","    validate_num = int(graph_region[-1] * validate_split)\n","    validate_mask = random.choices(remain_mask, k=validate_num)\n","    validate_set = set(validate_mask)\n","    validate_mask = list(validate_set)\n","    train_mask = [i for i in remain_mask if i not in validate_set]\n","    return test_mask, train_mask, validate_mask\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mw3RF0oyboXP"},"outputs":[],"source":["# import mydata\n","import matplotlib.pyplot as plt\n","import torch_geometric as pyg\n","import torch.nn as nn\n","import torch \n","from torch_geometric.nn import GCNConv, GATConv\n","import torch.nn.functional as F\n","from collections import defaultdict\n","import numpy as np\n","from torch_geometric.data import data as D \n","from tqdm.notebook import tqdm\n","import os \n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8q6VYDyRdwe5"},"outputs":[],"source":["\n","\n","class SingletonMeta(type):\n","    __instance = None\n","    def __call__(cls,*args,**kwargs):\n","        if not cls.__instance:\n","            cls.__instance = type.__call__(cls,*args,**kwargs)\n","        return cls.__instance\n","\n","class HugeLog():\n","\n","  def __init__(self,fpath=\"log.md\"):\n","\n","    self.f = open(fpath, 'w')\n","    self.content = defaultdict(list)\n","\n","  def write(self,content):\n","    if type(content) != str:\n","      self.f.write(str(content)+\"\\n\")\n","    else:\n","      self.f.write(content+\"\\n\")\n","    self.f.flush()\n","\n","  def appendImage(self, key, img):\n","    self.content[key].append(img)\n","\n","  def prefixWrite(self, content, prefix=\"[info]  \"):\n","\n","    if type(content) != str:\n","      self.write(prefix + str(content))\n","    else:\n","      self.write(prefix + content)\n","\n","\n","  def info(self,content):\n","\n","    self.prefixWrite(content, prefix=\"[info]. \")\n","\n","  def error(self, content):\n","    self.prefixWrite(content, prefix=\"[Error]. \")\n","    \n","\n","  def other(self,fun,content):\n","    self.write(fun(content))\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JC9l0Kp6en5D"},"outputs":[],"source":["\n","def preprocess_X(X,epsilon=0.00001):\n","    \n","    mean = X.mean()\n","    var  = X.var()\n","    X = (X - mean) / (var + epsilon)\n","    return X\n","\n","\n","def preprocess_Y(Y, weight_list=None):\n","  if weight_list:\n","    Y = Y / np.array(weight_list)\n","  else:\n","    weight_list = [100] + [50] * 17\n","    Y = Y / weight_list\n","  return Y\n","\n","\n","def preprocess_Y_Propobability_softmax(Y):\n","  \n","  temp = Y\n","  temp = np.exp(temp)\n","  temp = (temp.T / temp.sum(axis=-1)).T\n","  return temp\n","\n","\n","def preprocess_Y_Propobability(Y, theta=0.000001):\n","  \n","  temp = Y\n","  temp = (temp.T / (temp.sum(axis=-1) + theta)).T\n","  return temp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gwn8aQJqA4ci"},"outputs":[],"source":["def pyg_data_maker(x,y,edge_index, edge_attr, train_mask, val_mask, test_mask,\n","                   x_preprocess = True, ):\n","    \n","    data = D.Data()\n","\n","    if x_preprocess:\n","      x = preprocess_X(x)\n","\n","    y = preprocess_Y_Propobability(y)\n","\n","    x = torch.tensor(x,dtype=torch.float)\n","    y = torch.tensor(y,dtype=torch.float)\n","    edge_index = torch.tensor(edge_index.T,dtype=torch.long)\n","    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n","\n","    data.x, data.y, data.edge_index, data.edge_attr, data.train_mask, data.val_mask, data.test_mask =  x,y,edge_index, edge_attr, train_mask, val_mask, test_mask\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q2LN4cEd658E"},"outputs":[],"source":["def fake_graph_generate(nodes, nodes_shape):\n","  \n","  X = np.random.rand(nodes, nodes_shape)\n","  adj, adj_weight = get_adj(X,topk=5)\n","  return X, adj, adj_weight"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MNukxfqZboZy"},"outputs":[],"source":["# model\n","\n","\n","class GCN(nn.Module):\n","    # def __init__(self, nfeat=980, nhid=256, gnn_out=64, mlp_hidden=36, nout=18, dropout=0.1):\n","    # cell type: 38\n","    # def __init__(self, nfeat=979, nhid=256, gnn_out=64, mlp_hidden=36, nout=38, dropout=0.1):\n","\n","    # cell type: 19\n","    def __init__(self, nfeat=1000, nhid=256, gnn_out=64, mlp_hidden=36, nout=19, dropout=0.1):\n","\n","        super(GCN, self).__init__()\n","\n","        self.gc1 = GCNConv(nfeat, nhid)\n","        self.gc2 = GCNConv(nhid, gnn_out)\n","        \n","    def reset_parameters(self):\n","        self.gc1.reset_parameters()\n","        self.gc2.reset_parameters()\n","        # self.mlp.reset_parameters()\n","        \n","    def forward(self, data):\n","        x =  data.x\n","        adj = data.edge_index\n","        weight = data.edge_attr\n","        x = F.leaky_relu(self.gc1(x, adj, weight))\n","        x = self.gc2(x, adj, weight)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1aKcDYZULt35"},"outputs":[],"source":["\n","def saveResult(prefix):\n","  if os.path.isdir(prefix):\n","    pass\n","  else:\n","    os.mkdir(prefix)\n","\n","def savefig(prefix, content=['train','test','val','test-jsd','test-mse',\n","                             'test-mae','val-jsd','val-mse','val-mae',\n","                             \"train-mae\",'train-jsd','train-mse']):\n","  for item in content:\n","    fig = plt.figure()\n","    plt.plot(total_loss_collect[item])\n","    plt.title(prefix+\"/\"+item)\n","    plt.savefig(prefix+\"/\"+item+\".png\")\n","\n","\n","def logrecord(prefix=None):\n","\n","  content = ['test-jsd','test-mae','test-mse','test-pcc',\n","            'val-jsd','val-mae', 'val-mse', 'val-pcc',\n","            'train-jsd', 'train-mae','train-mse', 'train-pcc']\n","\n","  for item in content:\n","    mylog.info(\"min \" + item + \" : \" + str(min(total_loss_collect[item][:-2])))\n","    mylog.info(\"select\" + item + \" : \" + str(total_loss_collect[item][-1]))\n","\n","\n","def outputSave(prefix=None, data=None):\n","  # cell type: 38\n","  # columns = \"Ascending.vasa.recta.endothelium\tB-cell\tConnecting.tubule\tDescending.vasa.recta.endothelium\tDistinct.proximal.tubule.1\tDistinct.proximal.tubule.2\tEpithelial.progenitor.cell\tFibroblast\tGlomerular.endothelium\tIndistinct.intercalated.cell\tMNP.a.classical.monocyte.derived\tMNP.b.non.classical.monocyte.derived\tMNP.c.dendritic.cell\tMyofibroblast\tNK\tPelvic.epithelium\tPeritubular.capillary.endothelium.1\tPeritubular.capillary.endothelium.2\tPodocyte\tPrincipal.cell\tProliferating.Proximal.Tubule\tProximal.tubule\tT CD4 memory\tT CD4 naive\tT CD8 memory\tT CD8 naive\tThick.ascending.limb.of.Loop.of.Henle\tTransitional.urothelium\tTreg\tType.A.intercalated.cell\tType.B.intercalated.cell\tmDC\tmacrophage\tmast\tmonocyte\tneutrophil\tpDC\tplasmablast\".split(\"\t\")\n","  \n","  # cell type: 19\n","  columns = \"Antibody.secreting.B.cells\tCD3+.alpha.beta.T.cells\tCentral.venous.LSECs\tCholangiocytes\tErthyroid.cells\tHep.1\tHep.3\tHep.4\tHep.5\tHep.6\tInflammatory.macrophages\tMature.B.cells\tNK.like.cells\tNon.inflammatory.macrophages\tNotDet\tPeriportal.LSECs\tPortal.endothelial.cells\tStellate.cells\tgamma.delta.T.cells.1\".split(\"\t\")\n","\n","  data = F.softmax(model(data),dim=1).detach().cpu().numpy()\n","\n","  columns = columns[-1 * data.shape[-1]:]\n","  split_idx = [0,800,1600, len(data)]\n","  for idx in range(len(split_idx)-1):\n","    df = pd.DataFrame(data[split_idx[idx] : split_idx[idx+1]], columns=columns)\n","    df.to_csv(prefix+\"-predict-{}.csv\".format(idx))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SczeGALekSMN"},"outputs":[],"source":["# def jsd(p_output, q_output):\n","#     \"\"\"\n","#     Function that measures JS divergence between target and output logits:\n","#     \"\"\"\n","#     KLDivLoss = nn.KLDivLoss(reduction='batchmean')\n","#     log_mean_output = ((p_output + q_output )/2).log()\n","#     return (KLDivLoss(log_mean_output, p_output) + KLDivLoss(log_mean_output, q_output))/2\n","\n","# def mse(p, q):\n","#   return (p - q).pow(2).mean()\n","\n","# def mae(p, q):\n","#   return (p - q).abs().mean() \n","\n","# score_fns = [jsd, mse, mae]\n","def pcc(p, q):\n","    p_mean = torch.mean(p)\n","    q_mean = torch.mean(q)\n","    p_centered = p - p_mean\n","    q_centered = q - q_mean\n","    numerator = torch.sum(p_centered * q_centered)\n","    denominator = torch.sqrt(torch.sum(p_centered ** 2)) * torch.sqrt(torch.sum(q_centered ** 2))\n","    pearson_coef = numerator / denominator\n","    return torch.nan_to_num(pearson_coef)\n","\n","# def pcc(p, q):\n","#   from torchmetrics import PearsonCorrCoef\n","#   device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","#   pearson = PearsonCorrCoef().to(device)\n","#   pcc_list = []\n","\n","#   for i in range(len(p)):\n","#     # r = torch.nan_to_num(pearson(p[i], q[i]))\n","#     r = (pearson(p[i], q[i]))\n","#     pcc_list.append(r)\n","  \n","#   tensor_sum = torch.zeros_like(pcc_list[0])\n","#   for tensor in pcc_list:\n","#       tensor_sum += tensor\n","#   tensor_average = tensor_sum / len(pcc_list)\n","#   return tensor_average\n","\n","# def pcc(p, q):\n","#   p = p.detach().cpu().numpy()\n","#   q = q.detach().cpu().numpy()\n","#   pcc_list = []\n","#   from scipy.stats import pearsonr\n","#   for i in range(len(p)):\n","#       r = np.nan_to_num(pearsonr(p[i], q[i]))\n","#       pcc_list.append(r[0])\n","#   pcc_result = np.mean(pcc_list)\n","#   return pcc_result\n","\n","# def jsd(p, q):\n","#   p = p.detach().cpu().numpy()\n","#   q = q.detach().cpu().numpy()\n","\n","#   jsd_list = []\n","#   from scipy.spatial.distance import jensenshannon\n","#   for i in range(len(p)):\n","#     r = np.nan_to_num(jensenshannon(p[i], q[i]))\n","#     jsd_list.append(r)\n","#   jsd_result = np.mean(jsd_list)\n","#   return jsd_result\n","\n","def pcc(p, q):\n","    p_mean = torch.mean(p)\n","    q_mean = torch.mean(q)\n","    p_centered = p - p_mean\n","    q_centered = q - q_mean\n","    numerator = torch.sum(p_centered * q_centered)\n","    denominator = torch.sqrt(torch.sum(p_centered ** 2)) * torch.sqrt(torch.sum(q_centered ** 2))\n","    pearson_coef = numerator / denominator\n","    return torch.nan_to_num(pearson_coef)\n","    \n","def jsd(p_output, q_output):\n","  \"\"\"\n","  Function that measures JS divergence between target and output logits:\n","  \"\"\"\n","  KLDivLoss = nn.KLDivLoss(reduction='batchmean')\n","  log_mean_output = ((p_output + q_output )/2).log()\n","  jsd_result = (KLDivLoss(log_mean_output, p_output) + KLDivLoss(log_mean_output, q_output))/2\n","  return torch.nan_to_num(jsd_result)\n","\n","\n","def mse(p, q):\n","  return (p - q).pow(2).mean()\n","\n","def mae(p, q):\n","  return (p - q).abs().mean() \n","\n","score_fns = [jsd, mse, mae, pcc]\n","\n","# score_fns = [mse, mae]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"553RiXSxh-q8"},"outputs":[],"source":["def score_fn(input_, target, mask, softmax=True, prefix='test'):\n","  \n","  p = input_[mask]\n","\n","  if softmax:\n","    p = F.softmax(p, dim=1)\n","\n","  q = target[mask]\n","\n","  # print(\"p:\", p)\n","  # print(\"q:\", q)\n","\n","  # import ipdb\n","  # ipdb.set_trace()\n","\n","  for fn in score_fns:\n","    total_loss_collect[prefix + \"-\" +fn.__name__].append(fn(p, q).item())\n","  return torch.tensor(0)\n","\n","\n","def loss_fn(input_, target, mask):\n","  # input_logit = F.softmax(input_[mask],dim=1)\n","  # target_logit= F.softmax(target[mask],dim=1)\n","  input_logit = input_[mask]\n","  target_logit = target[mask]\n","  loss = F.cross_entropy(input_logit, target_logit)\n","  return loss \n","\n","\n","def test_with_masked_type(test_type=[0,1,2,3]):\n","  gnn_out= len(test_type)\n","  # gene eexpression dimension: 1000\n","  model = GCN(1000, gnn_out=gnn_out , nout=len(test_type))\n","  # print(model)\n","  model = fit_mask(data, model, 2000, lr=lr, weight_decay=weight_decay, test_type=test_type)\n","  return model\n","\n","def fit_mask(data, model, epoches, lr=0.0001, weight_decay=0.99, test_type=[0,1,2]):\n","  \n","  if torch.cuda.is_available():\n","    model = model.cuda()\n","    data = data.cuda()\n","    \n","  opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","  # print(\"learning parameters: lr\",lr)\n","  # train \n","  for epoch in tqdm(range(epoches)):\n","    train_mask_fn(data, model, opt, test_type)\n","    inference(data, model, test_type, prefix='val')\n","    inference(data, model, test_type, prefix='test')\n","    epoch_end(model)\n","\n","  model = project_end(model)\n","  inference(data, model, test_type, prefix='train')\n","  inference(data, model, test_type, prefix='val')\n","  inference(data, model, test_type, prefix='test')\n","  return model\n","\n","def epoch_end(model):\n","\n","  val_list = total_loss_collect['val-mse']\n","  if len(val_list) > 1:\n","    if min(val_list) != val_list[-1]:\n","      pass\n","    else:\n","      torch.save(model.state_dict(), \"best_model.pt\")\n","\n","  else:\n","    torch.save(model.state_dict(), \"best_model.pt\")\n","\n","def project_end(model):\n","  model.load_state_dict(torch.load(\"best_model.pt\"))\n","  return model\n","\n","\n","def train_mask_fn(data, model, opt, test_type=[0,1,2]):\n","  model.train()\n","  train_mask_ = data.train_mask\n","  label = data.y\n","  pred = model(data)\n","\n","  loss = loss_fn(pred, label[:,test_type], train_mask_)\n","  score= score_fn(pred, label[:,test_type], train_mask_, prefix='train')\n","\n","  total_loss_collect['train'].append(loss.item())\n","  opt.zero_grad()\n","  loss.backward()\n","  opt.step()\n","\n","\n","def inference(data, model, test_type=[0,1,2,3], prefix='val'):\n","  \n","  model.eval()\n","  mask = getattr(data, prefix+\"_mask\")\n","  label = data.y\n","  pred = model(data)\n","  loss = loss_fn(pred, label[:,test_type], mask)\n","  score= score_fn(pred, label[:,test_type], mask, prefix=prefix)\n","\n","  total_loss_collect[prefix].append(loss.item())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"liDyJ4XAgNUP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679178692456,"user_tz":420,"elapsed":12,"user":{"displayName":"jiayuan ding","userId":"00368278201421210170"}},"outputId":"cda755f8-7d6d-4cdb-f9ce-afee005967ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["!pwd"]},{"cell_type":"code","source":[],"metadata":{"id":"JGZf19z13XuZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GR-wYrio5kFF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lYaw6S1M3XxS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"avosIvBzguDH"},"outputs":[],"source":["mylog = HugeLog(fpath=\"log3.md\")\n","mylog.write(\"Data Description: \"+\"\\n All is TopK5, Propobability method : A/A.sum(-1)\")\n","mylog.write(\"All mask will be used!\")\n","\n","\n","# by default, top k (k=5) if threshold is not specified\n","# graph6 = sample_fovs_graph(\"/content/drive/MyDrive/CosMx_benchmark_pro/data/kidney/Run1080_SP19_1139/new/spot_gene_expression.csv\",\n","#                              \"/content/drive/MyDrive/CosMx_benchmark_pro/data/kidney/Run1080_SP19_1139/new/ground_truth.csv\", \n","#                            \"/content/drive/MyDrive/CosMx_benchmark_pro/data/kidney/Run1080_SP19_1139/new/spot_location.csv\",topk=5)\n","# spot_gene_expression = pd.read_csv(\"/content/drive/MyDrive/CosMx_benchmark_pro/data/kidney/Run1080_SP19_1139/new/spot_gene_expression.csv\")\n","# pseudo_spot_lst = [i for i in range(spot_gene_expression.shape[0])]\n","\n","\n","\n","\n","graph1 = sample_fovs_graph(\"/content/drive/MyDrive/CosMx_benchmark_pro/data/liver/health/new/samples/sample1/spot_gene_expression.csv\",\n","                      \"/content/drive/MyDrive/CosMx_benchmark_pro/data/liver/health/new/samples/sample1/spot_ground_truth.csv\",\n","                      \"/content/drive/MyDrive/CosMx_benchmark_pro/data/liver/health/new/samples/sample1/spot_location.csv\", topk=5)\n","\n","\n","graph2 = sample_fovs_graph(\"/content/drive/MyDrive/CosMx_benchmark_pro/data/liver/health/new/samples/sample2/spot_gene_expression.csv\",\n","                      \"/content/drive/MyDrive/CosMx_benchmark_pro/data/liver/health/new/samples/sample2/spot_ground_truth.csv\",\n","                      \"/content/drive/MyDrive/CosMx_benchmark_pro/data/liver/health/new/samples/sample2/spot_location.csv\", topk=5)\n","\n","graph3 = sample_fovs_graph(\"/content/drive/MyDrive/CosMx_benchmark_pro/data/liver/health/new/samples/sample3/spot_gene_expression.csv\",\n","                      \"/content/drive/MyDrive/CosMx_benchmark_pro/data/liver/health/new/samples/sample3/spot_ground_truth.csv\",\n","                      \"/content/drive/MyDrive/CosMx_benchmark_pro/data/liver/health/new/samples/sample3/spot_location.csv\", topk=5)\n","\n","\n","graph4 = sample_fovs_graph(\"/content/drive/MyDrive/CosMx_benchmark_pro/data/liver/health/new/samples/sample4/spot_gene_expression.csv\",\n","                      \"/content/drive/MyDrive/CosMx_benchmark_pro/data/liver/health/new/samples/sample4/spot_ground_truth.csv\",\n","                      \"/content/drive/MyDrive/CosMx_benchmark_pro/data/liver/health/new/samples/sample4/spot_location.csv\", topk=5)\n","\n","\n","\n","# total_graph = mydata.union_graph([graph1, graph2, graph3, graph4],topk=5)\n","total_graph = union_graph([graph1, graph2, graph3, graph4],topk=5)\n","\n","\n","# test_mask, train_mask, val_mask = mydata.mask_graph(total_graph, masked_graph_idx=masked_graph_idx)\n","\n","lrs = [0.01,0.05,0.001,0.005]\n","# lrs = [0.01]\n","\n","weight_decay_list = [0.01, 0.05, 0.001, 0.005, 0.0001]\n","# weight_decay_list = [0.01]\n","\n","\n","# masked_jobs = [[1]]\n","masked_jobs = [[0], [1], [2], [3]]\n","\n","\n","for lr in lrs:\n","  for masked_graph_idx in masked_jobs:\n","\n","    test_mask, train_mask, val_mask = mask_graph(total_graph, masked_graph_idx=masked_graph_idx)\n","    # import ipdb\n","    # ipdb.set_trace()\n","\n","    data = pyg_data_maker(total_graph.X, \n","                          total_graph.label, \n","                          total_graph.adj_link.T,\n","                          total_graph.adj_weight,\n","                          train_mask,\n","                          val_mask,\n","                          test_mask,\n","                          x_preprocess=False)\n","    \n","    mask_description = \"mask-{}\".format(\"-\".join(str(i) for i in masked_graph_idx))\n","\n","    # job1 \n","    for weight_decay in weight_decay_list:\n","      # test_type = list(range(18))\n","      # test_type = list(range(38)) # ground truth: cell type: 38\n","      test_type = list(range(19)) # ground truth: cell type: 19\n","      total_loss_collect = defaultdict(list) \n","\n","      model = test_with_masked_type(test_type=test_type)\n","\n","      prefix = \"{}-{}-lr-{}-weight-decay-{}\".format(mask_description,len(test_type), lr, weight_decay)\n","      mylog.write(\"# \"+ prefix)\n","      saveResult(prefix)\n","      savefig(prefix)\n","      logrecord()\n","      outputSave(prefix, data=data)\n","\n","    # job2 \n","    # for weight_decay in weight_decay_list:\n","    #   test_type = list(range(1,18))\n","    #   total_loss_collect = defaultdict(list) \n","\n","    #   model = test_with_masked_type(test_type=test_type)\n","\n","    #   prefix = \"{}-{}-lr-{}-weight-decay-{}\".format(mask_description,len(test_type), lr, weight_decay)\n","    #   mylog.write(\"# \"+ prefix)\n","    #   saveResult(prefix)\n","    #   savefig(prefix)\n","    #   logrecord()\n","    #   outputSave(prefix, data=data)\n","\n","m = open(\"log3.md\",'r').read().split('\\n')\n","content = defaultdict(list)\n","prefix = None\n","flag = True\n","columns = []\n","for idx in range(len(m)):\n","  if len(m[idx]) > 1:\n","    if m[idx][0] == \"#\":\n","      if prefix and flag:\n","        flag = False\n","      prefix = m[idx][2:]\n","      if \"log2\" in prefix:\n","        prefix = prefix[5:]\n","    elif prefix :\n","      if flag:\n","        columns.append(m[idx].split(\":\")[0])\n","      content[prefix].append(m[idx].split(\":\")[1])\n","klist = list(content.keys())\n","for i in klist:\n","  content[i] = content[i][:26]\n","df = pd.DataFrame(content).T\n","df.columns = columns\n","df.to_excel(\"res2.xlsx\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eg6asNbe383p"},"outputs":[],"source":["os.mkdir(\"result\") \n","!mv *.csv result\n","!mv *.xlsx result\n","!mv log3.md result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NixaHWrlQ-4e"},"outputs":[],"source":["!mv result liver_health\n","!cp -r ./liver_health /content/drive/MyDrive/CosMx_benchmark_pro/overall_result/liver/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-87ZSf4eQ-8H"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Ef0epQEQ-_A"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RrsIitD5hBky"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCSrJIKr1YHE"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2-W25mAfyWQ9"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQMJoCOgp56S"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-aPuESOxXF_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VPcN8JwOxbXy"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2_nYBnUizcOK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jivcQHXPzcLi"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tjk3xmfhxdqn"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i4OH-U1az3T1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zp-BR2L34bwl"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99V6_W4-4f9V"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}